# Claude Command Best Practices

## What Makes a Great Claude Command

### 1. Clear Context Setting
Start with role definition and expertise domain:
```
You are an expert in [DOMAIN] with deep knowledge of [SPECIFIC AREAS].
```

### 2. Specific Analytical Framework
Provide structured thinking patterns:
```
Analyze this through the lens of:
1. [Framework 1]
2. [Framework 2]
3. [Framework 3]
```

### 3. Concrete Success Metrics
Define what "good" looks like:
```
Industry standards for [DOMAIN]:
- Metric A: Target range X-Y
- Metric B: Minimum threshold Z
```

### 4. Multiple Output Formats
Request varied perspectives:
```
Provide analysis as:
1. Executive summary
2. Detailed breakdown
3. Actionable recommendations
4. Comparison table
```

### 5. Trigger Deep Thinking
Use specific keywords that activate thorough analysis:
- "Think deeply about..."
- "Cross-reference with best practices..."
- "Trace through all paths..."
- "Consider the psychological aspects..."
- "Analyze from the user's emotional journey..."

### 6. Request Specific Deliverables
Be explicit about outputs:
```
Create the following:
1. Comprehensive analysis report (ANALYSIS.md)
2. Gap identification document (GAPS.md)
3. Recommendations with priority (RECOMMENDATIONS.md)
4. Concise summary for stakeholders
```

### 7. Include Experience Mapping
Ask for user journey analysis:
```
Map the complete user experience:
- First 30 seconds
- First 5 minutes
- First hour
- Retention points
- Drop-off risks
```

### 8. Require Quantitative Analysis
Request specific measurements:
```
Compare against industry benchmarks:
- Current implementation: X
- Industry standard: Y
- Gap: Z%
- Priority: High/Medium/Low
```

### 9. Demand Both Problems AND Solutions
Don't just identify issues:
```
For each problem identified:
1. Current state
2. Ideal state
3. Gap analysis
4. Specific solution
5. Implementation effort
6. Expected impact
```

### 10. Use Emotional/Sensory Language
Engage different thinking modes:
```
Consider:
- What does the user FEEL?
- What creates DOPAMINE hits?
- Where are the SATISFACTION moments?
- What causes FRUSTRATION?
```

## Command Structure Template

```markdown
# [Domain] Analysis Command

## Role & Expertise
You are [ROLE] with expertise in [DOMAINS].

## Context
[Provide background about what's being analyzed]

## Analytical Framework
Analyze through these lenses:
1. [Lens 1 - e.g., "Fun mechanics"]
2. [Lens 2 - e.g., "User engagement"]
3. [Lens 3 - e.g., "Technical feasibility"]

## Required Analysis
1. **Current State Assessment**
   - What exists now
   - How it works
   - User experience walkthrough

2. **Best Practices Comparison**
   - Industry standards
   - Successful examples
   - Key metrics

3. **Gap Analysis**
   - What's missing
   - What's underperforming
   - Priority ranking

4. **Recommendations**
   - Quick wins
   - Medium-term improvements
   - Long-term vision

## Deliverables
1. Comprehensive report (REPORT.md)
2. Executive summary
3. Implementation roadmap
4. Metrics dashboard

## Success Criteria
The analysis should:
- Be actionable
- Include specific metrics
- Prioritize by impact
- Consider implementation effort
```

## Keywords That Trigger Deep Analysis

### Investigation Triggers
- "Research best practices for..."
- "Deep dive into..."
- "Thoroughly examine..."
- "Comprehensive analysis of..."

### Comparison Triggers
- "Cross-reference with industry standards"
- "Compare against successful examples"
- "Benchmark versus competitors"
- "Identify gaps between current and ideal"

### User-Centric Triggers
- "Trace the complete user journey"
- "Map the emotional experience"
- "Identify dopamine triggers"
- "Find frustration points"

### Solution-Oriented Triggers
- "Provide specific solutions"
- "Create actionable recommendations"
- "Design implementation roadmap"
- "Prioritize by impact and effort"

### Quantitative Triggers
- "Measure against metrics"
- "Calculate the gaps"
- "Quantify the improvements"
- "Provide numerical targets"

## Example: Highly Effective Command

```markdown
**ULTRATHINK** Do research on best practices for [DOMAIN]. 
Look deeply at the codebase and trace through all the paths 
and understand what the user can see and experience. 
Think deeply about how this cross-references with the research 
on [DOMAIN]. Use the other research/analysis files you just 
generated to inform your opinion, then create a comprehensive 
report. Then show me a concise summary.
```

This command works because it:
1. Triggers deep thinking mode ("ULTRATHINK")
2. Requests research first
3. Demands code examination
4. Requires user experience mapping
5. Asks for cross-referencing
6. Builds on previous analysis
7. Requests both detailed and summary outputs

## Anti-Patterns to Avoid

### Too Vague
❌ "Analyze this and tell me what you think"

### No Success Criteria
❌ "Make it better"

### Single Perspective
❌ "Just look at the technical aspects"

### No Actionable Output
❌ "Give me your opinion"

### Missing Context
❌ "Fix the problems"

## Measuring Command Effectiveness

A great command produces:
1. **Structured analysis** with clear sections
2. **Specific metrics** and measurements
3. **Prioritized recommendations** with effort estimates
4. **Multiple perspectives** (technical, user, business)
5. **Actionable next steps** that can be immediately implemented
6. **Clear documentation** that others can follow

## Tips for Iterative Refinement

1. Start broad, then narrow
2. Use findings from one analysis to inform the next
3. Request contrasting viewpoints
4. Ask for validation of assumptions
5. Build comprehensive understanding through multiple passes

Remember: The more specific and structured your command, the more valuable and actionable the output will be.