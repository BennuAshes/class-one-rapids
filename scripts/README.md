# Scripts Directory

## Feature to Code Workflow

### Quick Start

Run the unified workflow script (best practices enabled by default):

```bash
./scripts/feature-to-code-unified.sh "Your feature description"
```

This will:

- âœ… Generate PRD, Technical Design, and Task List
- âœ… Preserve full JSON metadata (conversation history, costs, tokens)
- âœ… Auto-extract readable documents
- âœ… Copy extracted docs to `docs/specs/[execution_id]/`
- âœ… Track workflow in Langfuse (if running)

### Environment Variables

All features are **enabled by default**. Use these to opt-out:

| Variable             | Default | Description                                      |
| -------------------- | ------- | ------------------------------------------------ |
| `WORKFLOW_ENGINE`    | `bash`  | Set to `python` to use Python workflow implementation |
| `DISABLE_TELEMETRY`  | `0`     | Set to `1` to disable Langfuse telemetry         |
| `DISABLE_EXTRACTION` | `0`     | Set to `1` to skip auto-extraction               |
| `SKIP_SPECS_COPY`    | `0`     | Set to `1` to keep docs only in workflow-outputs |
| `APPROVAL_MODE`      | `file`  | Set to `auto` or `interactive` (auto still requires Execute Tasks approval) |
| `APPROVAL_TIMEOUT`   | `0`     | Timeout in seconds (0 = unlimited)               |
| `AUTO_APPLY_FEEDBACK` | `false` | Set to `true` to automatically apply feedback improvements |
| `AUTO_RETRY_AFTER_FEEDBACK` | `false` | Set to `true` to retry steps after applying feedback |
| `SHOW_FILE_CHANGES` | `true`  | Set to `false` to disable file change tracking |
| `REQUIRE_COMMAND_APPROVAL` | `true` | Set to `false` to skip approval for command improvements |

### Examples

**Minimal observability (no telemetry, no specs copy):**

```bash
DISABLE_TELEMETRY=1 SKIP_SPECS_COPY=1 ./scripts/feature-to-code-unified.sh "feature"
```

**Auto-approve everything:**

```bash
APPROVAL_MODE=auto ./scripts/feature-to-code-unified.sh "feature"
```

**From a spec file:**

```bash
./scripts/feature-to-code-unified.sh docs/specs/my-feature/spec.md
```

**Resume existing workflow:**

```bash
./scripts/feature-to-code-unified.sh workflow-outputs/20251026_123456
```

## Utility Scripts

### Python Workflow (Recommended)

Use the new Python implementation for better reliability and features:

```bash
# Enable Python workflow engine
WORKFLOW_ENGINE=python ./scripts/feature-to-code-unified.sh "feature description"

# Or run directly
python3 -m scripts.workflow.cli "feature description"

# With auto-approval
python3 -m scripts.workflow.cli --approval-mode auto feature.md

# Interactive mode
python3 -m scripts.workflow.cli --approval-mode interactive feature.md
```

**Benefits:**
- âœ… Fixes stream-json output issues
- âœ… Type-safe implementation
- âœ… Better error handling
- âœ… Resumable workflows
- âœ… Async I/O for better performance

### Extract Artifacts

Extract readable documents from JSON streaming output:

```bash
# Extract in place (creates .extracted.md files)
python3 scripts/utils/extract-artifacts.py workflow-outputs/20251026_123456

# Extract to specific location
python3 scripts/utils/extract-artifacts.py workflow-outputs/20251026_123456 docs/specs/my-feature
```

### Claude with Telemetry

Wrapper for `claude -p` that adds Langfuse telemetry:

```bash
python3 scripts/utils/claude-with-telemetry.py "Your prompt" "execution_id" "step_name"
```

### Check Approvals

Diagnostic tool for troubleshooting approval files:

```bash
python3 scripts/utils/check-approvals.py workflow-outputs/20251026_123456
```

## Workflow Approval System

The workflow includes approval checkpoints at:

1. After PRD generation
2. After Technical Design generation
3. After Task List generation

### Approval Modes

**File-based (default):**

- Script creates `.approval_*.json` files
- Respond with `.approval_*.json.response` files
- Approve: `echo '{"status":"approved"}' > .approval_PRD.json.response`
- Reject: `echo '{"status":"rejected","reason":"..."}' > .approval_PRD.json.response`

**Interactive:**

- Prompts in terminal for y/n
- Set with: `APPROVAL_MODE=interactive`

**Auto:**

- Automatically approves all checkpoints EXCEPT "Execute Tasks"
- Set with: `APPROVAL_MODE=auto`
- **âš ï¸ Important:** The "Execute Tasks" approval always requires explicit user confirmation, even in auto mode

### Special: Execute Tasks Approval

The **"Execute Tasks"** checkpoint is a special safety measure that:
- **Always requires explicit approval**, regardless of approval mode
- Appears after the Task List is approved
- Is visually highlighted in both UIs with:
  - Red borders and pulsing animation
  - "âš ï¸ FINAL APPROVAL" badge
  - Warning text about code execution
- Triggers actual code generation and test execution via `/execute-task` command
- Cannot be bypassed with `APPROVAL_MODE=auto`

This ensures users always have final control before code execution begins.

### Automatic Feedback Application

When workflows are rejected with feedback, the system can automatically improve the commands and retry:

**Enable Auto-Apply Feedback:**
```bash
AUTO_APPLY_FEEDBACK=true ./scripts/feature-to-code-unified.sh "feature"
```

**Enable Auto-Retry After Feedback:**
```bash
AUTO_APPLY_FEEDBACK=true AUTO_RETRY_AFTER_FEEDBACK=true ./scripts/feature-to-code-unified.sh "feature"
```

**How it works:**
1. When a checkpoint is rejected with feedback
2. The `/reflect` command analyzes the feedback
3. If `AUTO_APPLY_FEEDBACK=true`, `/flow:apply-reflect` updates the command
4. If `AUTO_RETRY_AFTER_FEEDBACK=true`, the step is retried with the improved command
5. Otherwise, workflow stops for manual review

### Command Improvement Approval

When feedback is applied to improve commands, the system can require approval before modifying the flow commands:

**Features:**
- When `REQUIRE_COMMAND_APPROVAL=true` (default), command changes require explicit approval
- Shows a diff of proposed changes to `.claude/commands/flow/*.md` files
- Displays context about why changes are needed (original feedback)
- Lists specific improvements being made
- Special purple/blue theme in UI to distinguish from regular approvals
- "ğŸ”§ COMMAND UPDATE" badge for easy identification

**Enable Command Approval (default):**
```bash
AUTO_APPLY_FEEDBACK=true REQUIRE_COMMAND_APPROVAL=true ./scripts/feature-to-code-unified.sh "feature"
```

**Skip Command Approval (trust automatic improvements):**
```bash
AUTO_APPLY_FEEDBACK=true REQUIRE_COMMAND_APPROVAL=false ./scripts/feature-to-code-unified.sh "feature"
```

**Workflow with Command Approval:**
1. Checkpoint gets rejected with feedback
2. System runs `/reflect` to generate improvement suggestions
3. **NEW**: System generates proposed command changes (dry-run)
4. **NEW**: Creates approval request showing command diff
5. **NEW**: You review the exact changes that will be made
6. If approved: Command file is updated with improvements
7. Optionally: System retries the step with improved command

This gives you full visibility and control over how your flow commands are modified based on feedback.

### File Change Tracking

The approval system now shows which files will be created or modified:

**Features:**
- Shows list of changed files with status (created/modified/deleted)
- Displays hierarchical file tree structure
- Includes git diff preview for code changes
- Visible in both web UI and mobile app

**Disable if not needed:**
```bash
SHOW_FILE_CHANGES=false ./scripts/feature-to-code-unified.sh "feature"
```

**Implementation:**
- Uses git to track changes in workflow directory
- Commits after each checkpoint for accurate tracking
- File changes included in approval JSON
- UI components display file tree with status badges

## Observability

### Full Stack (Langfuse + Approval Server)

The observability stack includes both Langfuse telemetry and the workflow approval server:

1. Start stack: `cd observability && docker-compose up -d`
2. Run workflow normally
3. View traces at http://localhost:3000 (Langfuse)
4. **Manage approvals at http://localhost:8080 (Web Dashboard)**

The approval server runs automatically as part of the docker-compose stack and includes a full web UI for managing workflow approvals. Just open http://localhost:8080 in your browser!

### Langfuse Integration

When telemetry is enabled (default):

1. Tracks each workflow step (PRD, Design, Tasks)
2. Records approval checkpoints with timing
3. Logs model usage and costs
4. Captures custom metadata

### What Gets Tracked

- Each workflow step (PRD, Design, Tasks)
- Approval checkpoints with duration
- Timing and performance metrics
- Model usage and costs
- Custom metadata

## Migration from Old Script

The original `feature-to-code.sh` is deprecated and redirects to the unified script. See `FEATURE_TO_CODE_MIGRATION.md` for details.

## Troubleshooting

**Langfuse not running:**

- Telemetry fails gracefully, workflow continues
- Disable with: `DISABLE_TELEMETRY=1`

**Python not found:**

- Extraction and telemetry require Python 3
- Install: `apt install python3` or `brew install python3`

**Langfuse SDK missing:**

- Quick install: `./scripts/install-telemetry-deps.sh` (tries multiple methods)
- Manual install: `python3 -m pip install --user langfuse`
- For externally-managed environments: See installation script output for options

**Approval timeout:**

- Set timeout: `APPROVAL_TIMEOUT=300` (5 minutes)
- Default is unlimited (0)

## File Structure

```
scripts/
â”œâ”€â”€ feature-to-code-unified.sh      # Main CLI entry point (delegates to Python or runs bash)
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ PYTHON_INSTALL_GUIDE.md         # Python setup guide
â”œâ”€â”€ WORKFLOW_APPROVALS.md           # Approval system reference
â”‚
â”œâ”€â”€ workflow/                       # Python workflow implementation (NEW)
â”‚   â”œâ”€â”€ cli.py                      # Command-line interface
â”‚   â”œâ”€â”€ orchestrator.py             # Main workflow coordination
â”‚   â”œâ”€â”€ types.py                    # Type definitions
â”‚   â”œâ”€â”€ services/                   # Reusable services
â”‚   â”‚   â”œâ”€â”€ claude_cli.py           # Claude subprocess wrapper (with stream-json)
â”‚   â”‚   â”œâ”€â”€ approval.py             # Approval flows (file/auto/interactive)
â”‚   â”‚   â”œâ”€â”€ telemetry.py            # Langfuse telemetry integration
â”‚   â”‚   â””â”€â”€ artifact_extraction.py  # Artifact extraction service
â”‚   â”œâ”€â”€ steps/                      # Workflow step executors
â”‚   â”‚   â”œâ”€â”€ prd.py                  # PRD generation
â”‚   â”‚   â”œâ”€â”€ design.py               # Technical design generation
â”‚   â”‚   â”œâ”€â”€ tasks.py                # Task list generation
â”‚   â”‚   â”œâ”€â”€ execute.py              # Task execution
â”‚   â”‚   â””â”€â”€ summary.py              # Summary generation
â”‚   â””â”€â”€ utils/                      # Utilities
â”‚       â””â”€â”€ file_ops.py             # Async file operations
â”‚
â”œâ”€â”€ utils/                          # Standalone utilities
â”‚   â”œâ”€â”€ extract-artifacts.py        # Extract docs from JSON
â”‚   â”œâ”€â”€ claude-with-telemetry.py    # Claude CLI wrapper with telemetry
â”‚   â”œâ”€â”€ check-approvals.py          # Diagnostic tool for approval files
â”‚   â””â”€â”€ track-workflow-step.py      # Manual workflow step tracking
â”‚
â”œâ”€â”€ langfuse/                       # Langfuse-specific libraries
â”‚   â”œâ”€â”€ judge_prompts.py            # LLM judge prompts
â”‚   â”œâ”€â”€ langfuse_evaluators.py      # Evaluation functions
â”‚   â””â”€â”€ setup_langfuse_score_configs.py  # Scoring configuration
â”‚
â”œâ”€â”€ setup/                          # Setup & diagnostic scripts
â”‚   â”œâ”€â”€ install-telemetry-deps.sh   # Install Langfuse SDK dependencies
â”‚   â”œâ”€â”€ verify-telemetry-setup.sh   # Check telemetry configuration
â”‚   â”œâ”€â”€ setup-telemetry-env.sh      # Configure environment variables
â”‚   â””â”€â”€ test-telemetry.sh           # Test telemetry connectivity
â”‚
â”œâ”€â”€ libs/                           # Shared libraries
â”‚   â””â”€â”€ workflow_telemetry.py       # Telemetry library (used by bash)
â”‚
â”œâ”€â”€ deprecated/                     # Archived scripts (see deprecated/README.md)
â”‚   â”œâ”€â”€ feature-to-code.sh
â”‚   â”œâ”€â”€ feature-to-code-traced.sh
â”‚   â”œâ”€â”€ send-workflow-telemetry.sh
â”‚   â””â”€â”€ workflow-approval-ui.html
â”‚
â””â”€â”€ docs/                           # Archived documentation (see docs/README.md)
    â”œâ”€â”€ FEATURE_TO_CODE_MIGRATION.md
    â”œâ”€â”€ APPROVAL_SYSTEM_FIXES.md
    â””â”€â”€ PERFORMANCE_IMPROVEMENTS.md

(Root level)
â”œâ”€â”€ workflow-approval-server.py     # Approval UI server (runs in docker-compose)
â””â”€â”€ Dockerfile.approval-server      # Docker image for approval server
```

## Best Practices

1. **Keep JSON files** - They contain valuable metadata
2. **Use telemetry** - Track workflow performance and costs
3. **Review extracted docs** - JSON is for machines, extracted is for humans
4. **Organize in docs/specs** - Default behavior copies there
5. **Use execution IDs** - Link all artifacts and traces together
