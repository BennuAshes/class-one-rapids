"""
Mock LLM service for fast workflow testing.

Provides realistic mock responses for each workflow step without
calling actual LLM APIs. Perfect for testing the workflow logic,
caching, approval system, and parallel execution.
"""

import asyncio
from pathlib import Path
from typing import Optional
from datetime import datetime


class MockLLMService:
    """
    Mock service that simulates LLM responses for workflow steps.

    Features:
    - Realistic response templates for each step
    - Configurable delay to simulate API latency
    - Response variation for testing
    - Deterministic for cache testing
    """

    def __init__(self, delay_seconds: float = 0.1, enable_variation: bool = False):
        """
        Initialize mock LLM service.

        Args:
            delay_seconds: Simulated API latency (default: 0.1s)
            enable_variation: Add random variation to responses
        """
        self.delay_seconds = delay_seconds
        self.enable_variation = enable_variation
        self.call_count = {}

    async def generate_prd(self, feature_description: str) -> str:
        """Generate mock PRD."""
        await asyncio.sleep(self.delay_seconds)

        self.call_count['prd'] = self.call_count.get('prd', 0) + 1

        variation = f" (v{self.call_count['prd']})" if self.enable_variation else ""

        return f"""# Product Requirements Document{variation}

## Feature Overview
{feature_description}

## Objectives
1. Deliver a high-quality implementation of the requested feature
2. Ensure proper error handling and validation
3. Include comprehensive test coverage
4. Follow best practices and coding standards

## User Stories
- **As a user**, I want to use the feature described above
- **As a developer**, I want clear implementation requirements
- **As a tester**, I want testable acceptance criteria

## Functional Requirements

### Core Functionality
1. Implement the main feature as described
2. Include proper input validation
3. Handle edge cases gracefully
4. Return appropriate responses

### Non-Functional Requirements
- **Performance**: Response time < 100ms for typical requests
- **Reliability**: 99.9% uptime
- **Security**: Input validation, rate limiting, authentication
- **Scalability**: Support 1000 concurrent users

## Technical Considerations
- Use modern frameworks and libraries
- Follow RESTful API design principles
- Implement proper logging and monitoring
- Include health check endpoints

## Acceptance Criteria
- [ ] All functional requirements implemented
- [ ] Unit tests with >80% coverage
- [ ] Integration tests passing
- [ ] Documentation complete
- [ ] Code review approved

## Out of Scope
- Advanced analytics (Phase 2)
- Multi-language support (Phase 2)
- Advanced caching strategies (Phase 2)

## Timeline
- Development: 2 weeks
- Testing: 1 week
- Deployment: 3 days

---
*Generated by Mock LLM Service - {datetime.now().isoformat()}*
*Call #{self.call_count['prd']}*
"""

    async def generate_design(self, prd_content: str) -> str:
        """Generate mock Technical Design Document."""
        await asyncio.sleep(self.delay_seconds)

        self.call_count['design'] = self.call_count.get('design', 0) + 1

        variation = f" (v{self.call_count['design']})" if self.enable_variation else ""

        return f"""# Technical Design Document{variation}

## Architecture Overview

### System Components
```
┌─────────────┐      ┌──────────────┐      ┌─────────────┐
│   Client    │─────▶│  API Server  │─────▶│  Database   │
└─────────────┘      └──────────────┘      └─────────────┘
```

### Technology Stack
- **Backend**: Node.js with Express.js
- **Database**: PostgreSQL
- **Testing**: Jest
- **Deployment**: Docker containers

## API Design

### Endpoints

#### Main Endpoints
```
POST   /api/v1/resource
GET    /api/v1/resource/:id
PUT    /api/v1/resource/:id
DELETE /api/v1/resource/:id
```

#### Request/Response Format
```json
{{
  "request": {{
    "data": "value"
  }},
  "response": {{
    "success": true,
    "data": {{}},
    "timestamp": "2025-01-01T00:00:00Z"
  }}
}}
```

## Data Models

### Primary Entity
```javascript
{{
  id: String (UUID),
  name: String,
  description: String,
  createdAt: DateTime,
  updatedAt: DateTime,
  metadata: Object
}}
```

## Implementation Details

### Key Algorithms
1. **Validation**: Use JSON Schema for input validation
2. **Authorization**: JWT-based authentication
3. **Error Handling**: Centralized error middleware
4. **Logging**: Winston for structured logging

### Error Handling Strategy
```javascript
try {{
  // Operation
}} catch (error) {{
  logger.error(error);
  return res.status(500).json({{
    error: 'Internal server error',
    requestId: req.id
  }});
}}
```

## Security Considerations
- Input sanitization
- Rate limiting (100 req/min per IP)
- CORS configuration
- Helmet.js for HTTP headers
- SQL injection prevention

## Performance Optimization
- Database indexing on frequently queried fields
- Response caching (Redis)
- Connection pooling
- Async/await for non-blocking I/O

## Testing Strategy
- Unit tests for business logic
- Integration tests for API endpoints
- End-to-end tests for critical flows
- Load testing with Artillery

## Deployment Architecture
```
Load Balancer
    ├─ App Server 1
    ├─ App Server 2
    └─ App Server 3
          │
    PostgreSQL (Primary + Replica)
```

## Monitoring & Observability
- Prometheus metrics
- Grafana dashboards
- Error tracking with Sentry
- Structured logging

---
*Generated by Mock LLM Service - {datetime.now().isoformat()}*
*Call #{self.call_count['design']}*
"""

    async def generate_tasks(self, design_content: str) -> str:
        """Generate mock Task List."""
        await asyncio.sleep(self.delay_seconds)

        self.call_count['tasks'] = self.call_count.get('tasks', 0) + 1

        variation = f" (v{self.call_count['tasks']})" if self.enable_variation else ""

        return f"""# Implementation Task List{variation}

## Phase 1: Setup & Infrastructure

### Task 1.1: Project Setup
**Description**: Initialize project structure and dependencies
**Estimated Time**: 2 hours
**Dependencies**: None
**Acceptance Criteria**:
- [ ] Package.json configured
- [ ] ESLint and Prettier set up
- [ ] Git repository initialized
- [ ] README with setup instructions

### Task 1.2: Database Setup
**Description**: Set up PostgreSQL database and migrations
**Estimated Time**: 3 hours
**Dependencies**: Task 1.1
**Acceptance Criteria**:
- [ ] Database schema defined
- [ ] Migration scripts created
- [ ] Seed data for development
- [ ] Database connection pooling configured

## Phase 2: Core Implementation

### Task 2.1: API Server Setup
**Description**: Set up Express.js server with middleware
**Estimated Time**: 4 hours
**Dependencies**: Task 1.1
**Acceptance Criteria**:
- [ ] Express server configured
- [ ] Middleware stack set up (CORS, helmet, etc.)
- [ ] Error handling middleware
- [ ] Health check endpoint

### Task 2.2: Implement Main Endpoints
**Description**: Create CRUD endpoints for main resource
**Estimated Time**: 8 hours
**Dependencies**: Task 2.1, Task 1.2
**Acceptance Criteria**:
- [ ] POST endpoint implemented
- [ ] GET endpoint implemented
- [ ] PUT endpoint implemented
- [ ] DELETE endpoint implemented
- [ ] Input validation working

### Task 2.3: Authentication & Authorization
**Description**: Implement JWT-based authentication
**Estimated Time**: 6 hours
**Dependencies**: Task 2.1
**Acceptance Criteria**:
- [ ] JWT token generation
- [ ] Token validation middleware
- [ ] Protected routes working
- [ ] Token refresh mechanism

## Phase 3: Testing

### Task 3.1: Unit Tests
**Description**: Write unit tests for business logic
**Estimated Time**: 8 hours
**Dependencies**: Task 2.2
**Acceptance Criteria**:
- [ ] >80% code coverage
- [ ] All edge cases tested
- [ ] Mocks for external dependencies
- [ ] Tests passing in CI

### Task 3.2: Integration Tests
**Description**: Write integration tests for API endpoints
**Estimated Time**: 6 hours
**Dependencies**: Task 2.2
**Acceptance Criteria**:
- [ ] All endpoints tested
- [ ] Database transactions tested
- [ ] Error scenarios covered
- [ ] Test data cleanup working

### Task 3.3: End-to-End Tests
**Description**: Implement E2E test suite
**Estimated Time**: 4 hours
**Dependencies**: Task 3.2
**Acceptance Criteria**:
- [ ] Critical user flows tested
- [ ] Realistic test scenarios
- [ ] Automated E2E pipeline
- [ ] Screenshots on failure

## Phase 4: Documentation & Deployment

### Task 4.1: API Documentation
**Description**: Generate API documentation with Swagger
**Estimated Time**: 3 hours
**Dependencies**: Task 2.2
**Acceptance Criteria**:
- [ ] Swagger/OpenAPI spec complete
- [ ] All endpoints documented
- [ ] Request/response examples
- [ ] Interactive API explorer

### Task 4.2: Docker Setup
**Description**: Create Docker configuration
**Estimated Time**: 4 hours
**Dependencies**: Task 2.2
**Acceptance Criteria**:
- [ ] Dockerfile optimized
- [ ] Docker-compose for local dev
- [ ] Multi-stage build working
- [ ] Environment variables configured

### Task 4.3: CI/CD Pipeline
**Description**: Set up GitHub Actions for CI/CD
**Estimated Time**: 5 hours
**Dependencies**: Task 4.2
**Acceptance Criteria**:
- [ ] Automated tests on PR
- [ ] Linting checks
- [ ] Build verification
- [ ] Deployment pipeline

## Total Estimated Time: 53 hours (~7 working days)

---
*Generated by Mock LLM Service - {datetime.now().isoformat()}*
*Call #{self.call_count['tasks']}*
"""

    async def generate_summary(self, workflow_data: str) -> str:
        """Generate mock workflow summary."""
        await asyncio.sleep(self.delay_seconds)

        self.call_count['summary'] = self.call_count.get('summary', 0) + 1

        return f"""# Workflow Execution Summary

## Overview
Successfully completed the feature-to-code workflow for the requested feature.

## Artifacts Generated
1. ✅ Product Requirements Document (PRD)
2. ✅ Technical Design Document (TDD)
3. ✅ Implementation Task List
4. ✅ Execution Results

## Key Metrics
- **Total Steps**: 5
- **Steps Completed**: 5
- **Success Rate**: 100%
- **Total Duration**: ~{self.delay_seconds * 5:.1f} seconds (mock mode)

## Next Steps
1. Review generated artifacts
2. Execute implementation tasks
3. Set up development environment
4. Begin Phase 1 implementation

## Quality Indicators
- ✅ All requirements captured in PRD
- ✅ Technical design is comprehensive
- ✅ Task list is actionable and estimated
- ✅ No critical issues identified

---
*Generated by Mock LLM Service - {datetime.now().isoformat()}*
*Mock mode enabled - responses are simulated*
*Total LLM calls: {sum(self.call_count.values())}*
"""

    def get_stats(self) -> dict:
        """Get mock service statistics."""
        return {
            "total_calls": sum(self.call_count.values()),
            "calls_by_type": self.call_count,
            "delay_seconds": self.delay_seconds,
            "variation_enabled": self.enable_variation
        }


# Mock step executors that use the mock service

class MockStepExecutor:
    """Base class for mock step executors."""

    def __init__(self, mock_service: MockLLMService):
        self.mock_service = mock_service


async def mock_execute_prd(state, mock_service: MockLLMService):
    """Execute PRD step with mock service."""
    from ..types import StepResult
    from ..utils.file_ops import async_write_file
    from pathlib import Path
    from datetime import datetime

    # Generate mock response
    content = await mock_service.generate_prd(state.config.feature_description)

    # Save to file
    output_file = state.config.work_dir / f"prd_mock_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    await async_write_file(output_file, content)

    return StepResult(
        success=True,
        output_file=output_file,
        duration_seconds=mock_service.delay_seconds,
        metadata={"mock": True, "call_number": mock_service.call_count.get('prd', 1)}
    )


async def mock_execute_design(state, mock_service: MockLLMService):
    """Execute Design step with mock service."""
    from ..types import StepResult, StepName
    from ..utils.file_ops import async_write_file, async_read_file
    from pathlib import Path
    from datetime import datetime

    # Get PRD content
    prd_step = state.get_step(StepName.GENERATE_PRD)
    prd_content = ""
    if prd_step and prd_step.output_file and prd_step.output_file.exists():
        prd_content = await async_read_file(prd_step.output_file)

    # Generate mock response
    content = await mock_service.generate_design(prd_content)

    # Save to file
    output_file = state.config.work_dir / f"tdd_mock_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    await async_write_file(output_file, content)

    return StepResult(
        success=True,
        output_file=output_file,
        duration_seconds=mock_service.delay_seconds,
        metadata={"mock": True, "call_number": mock_service.call_count.get('design', 1)}
    )


async def mock_execute_tasks(state, mock_service: MockLLMService):
    """Execute Tasks step with mock service."""
    from ..types import StepResult, StepName
    from ..utils.file_ops import async_write_file, async_read_file
    from pathlib import Path
    from datetime import datetime

    # Get Design content
    design_step = state.get_step(StepName.GENERATE_DESIGN)
    design_content = ""
    if design_step and design_step.output_file and design_step.output_file.exists():
        design_content = await async_read_file(design_step.output_file)

    # Generate mock response
    content = await mock_service.generate_tasks(design_content)

    # Save to file
    output_file = state.config.work_dir / f"tasks_mock_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    await async_write_file(output_file, content)

    return StepResult(
        success=True,
        output_file=output_file,
        duration_seconds=mock_service.delay_seconds,
        metadata={"mock": True, "call_number": mock_service.call_count.get('tasks', 1)}
    )


async def mock_execute_summary(state, mock_service: MockLLMService):
    """Execute Summary step with mock service."""
    from ..types import StepResult
    from ..utils.file_ops import async_write_file
    from pathlib import Path
    from datetime import datetime

    # Generate mock response
    content = await mock_service.generate_summary("workflow data")

    # Save to file
    output_file = state.config.work_dir / f"summary_mock_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    await async_write_file(output_file, content)

    return StepResult(
        success=True,
        output_file=output_file,
        duration_seconds=mock_service.delay_seconds,
        metadata={"mock": True, "call_number": mock_service.call_count.get('summary', 1)}
    )


async def mock_execute_step(state, step_name):
    """Execute any step with mock service."""
    from ..types import StepName

    # Get or create mock service
    mock_service = MockLLMService(delay_seconds=0.1, enable_variation=False)

    if step_name == StepName.GENERATE_PRD:
        return await mock_execute_prd(state, mock_service)
    elif step_name == StepName.GENERATE_DESIGN:
        return await mock_execute_design(state, mock_service)
    elif step_name == StepName.GENERATE_TASKS:
        return await mock_execute_tasks(state, mock_service)
    elif step_name == StepName.GENERATE_SUMMARY:
        return await mock_execute_summary(state, mock_service)
    elif step_name == StepName.EXECUTE_TASKS:
        # Skip actual execution in mock mode
        from ..types import StepResult
        from ..utils.file_ops import async_write_file
        from datetime import datetime

        content = "# Mock Task Execution\n\nAll tasks completed successfully (mock mode)"
        output_file = state.config.work_dir / f"execute_mock_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        await async_write_file(output_file, content)

        return StepResult(
            success=True,
            output_file=output_file,
            duration_seconds=0.1,
            metadata={"mock": True, "skipped_execution": True}
        )

    # Unknown step
    from ..types import StepResult
    return StepResult(
        success=False,
        error_message=f"Unknown step in mock mode: {step_name}"
    )